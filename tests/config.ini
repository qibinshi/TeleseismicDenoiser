[directories]
# Directories with high capacity for data and model.
storage_home = /fd1/QibinShi_data/

# Waveform data and model folder under `storage_home/`.
data_dir = matfiles_for_denoiser/
save_model_dir = my_model/
;save_model_dir = Release_Middle_augmentation_P4Hz_150s
;save_model_dir = Release_Middle_augmentation_S4Hz_150s_removeCoda_SoverCoda20_1980_2021_include_noFMS

# Folder to save your results `storage_home/result_dir`
result_dir = DenoTe_results/testcode/



[data]
# Data should be stored under `storage_home/data_dir/`.
# our data are truncated based on phase types:
# P -- 50,000 and S -- 40,000 points
# modify half_length to match your own `data_file`.
half_length = 25000
data_file = Psnr25_lp4_2000-2021.hdf5
;half_length = 20000
;data_file = Ssnr20_lp4_1980_2021_include_noFMS.hdf5

# Or, use demo data in the package for training and testing.
# set 1 to use demo data, set 0 to use the `data_file`.
use_demo = 1

# Input size of the neural net
# our pretrained `Denote` model has input size of 1500.
# after training a new model, use the same `npts` for testing.
npts = 1500

# Maximum squeezing (stretching) ratio.
# make sure stretch_max * npts < half_length.
# default value 6 is based on P-S differential time.
stretch_max = 6

# Keys for loading the label quake and noise waveforms.
# modify them if you use different keys for your H5py data.
branch_signal = quake
branch_noise = noise

# Splitting ratio of data during training
# e.g., train=60%; validation=(1-train)*50%; test=(1-train)*50%
train_size = 0.6
test_size = 0.5

# Random seeds for shuffled splitting
rand_seed1 = 43
rand_seed2 = 11



[training]
# Master GPU index.
# to use CPU, use integer bigger than the total GPU number (9).
gpu = 0

# Multiple GPUs for parallel training
gpu_ids = 0,1,2,3

# Transfer learning options:
# 0 -- wrap layers around WaveDecompNet kernel (scratch)
# 1 -- from pretrained DenoTe (already wrapped)
transfer = 0

# Hyper-parameters for gradient descent
batch_size = 64
epochs = 200
learning_rate = 0.001
minimum_epochs = 30
patience = 40



[testing]
# Test the model on one batch of data
batch_size = 10

# Option of models
# 0 -- pretrained Denote model (in the package)
# 1 -- new model that you trained
#      (`storage_home/save_model_dir/retrained_weights`)
retrain = 0
retrained_weights = Branch_Encoder_Decoder_LSTM_weights.pth



[prediction]
# Where to find you data for application: under `storage_home/data_dir/`
data_wave = M6_deep100km_P.hdf5
data_meta = metadata_M6_deep100km_P.csv

# Or, use demo noisy data in the package application.
# set 1 to use demo data, set 0 to use the `data_wave`.
use_demo = 1

# Keys for loading your noisy waveform in H5py format
# default `quake` is used for our demo noisy data
data_key =quake

# Start of the time window to be denoised
# default 750 is used for our demo noisy data
start_point = 750

# Choose one recording of 3-component waveform to visualize
# make sure the index does not exceed the total size
sample_index = 89
;sample_index = 6





##### Below is used for our deep earthquake study
;# Window length for source time function study
;npts_trim = 600
;
;# t* from Poli and Prieto (2016)
;tstar = 0.3
;
;# 2-Z component, 1-N, 0-E
;component = 2
;
;# Lowest SNR to select raw data
;minsnr = 2
;
;# Number of threads to process many earthquakes
;threads = 2
;
;# Number of example events to be denoised
;num_event = 2